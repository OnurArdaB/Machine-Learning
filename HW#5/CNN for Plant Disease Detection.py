# -*- coding: utf-8 -*-
"""hw5_NN_toSolve.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1frkqLKQCD8FRCVH3THB8PtYvRMtP0PSc

### Make sure GPU is on

https://medium.com/deep-learning-turkey/google-colab-free-gpu-tutorial-e113627b9f5d
"""

import tensorflow as tf
tf.test.gpu_device_name()

import tensorflow_datasets as tfds
import random 
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from skimage.util import montage

"""### (10 pts.) Prepare the datasets

#### load the plant_village data from tensorflow_datasets. Split by %80-10-10 train-val-test using split= input.

####Â Please check https://www.tensorflow.org/datasets/splits
"""

raw_test,test_info = tfds.load('plant_village', split='train[:10%]',as_supervised=True,with_info=True)#### fill here
raw_val = tfds.load('plant_village', split='train[10%:20%]',as_supervised=True)#### fill here
raw_train = tfds.load('plant_village',split='train[20%:]',as_supervised=True) #### fill here

"""#### Normalize each image into [0,1] range, shuffle and minibatch 128."""

pp_test = raw_test.map(lambda img, label:(tf.image.convert_image_dtype(img/255, tf.float32), label)).shuffle(1024).batch(128)
pp_val = raw_val.map(lambda img,label:(tf.image.convert_image_dtype(img/255, tf.float32), label)).shuffle(1024).batch(128)
pp_train = raw_train.map(lambda img,label:(tf.image.convert_image_dtype(img/255, tf.float32), label)).shuffle(1024).batch(128)

"""#### Display the examples from the dataset."""

plt.figure(figsize=(10,10))
for example in pp_train.take(1):  # Only take a single example
  image, label = example[0], example[1]
  plt.imshow(montage(image, multichannel=True))

"""### (20 pts) The Fully Connected Neural Network implementation

#### It is a 4 layer (Fully Connected) Neural Network. The feature depths are [512,256,128] and last softmax layer has 38 output. Train for 10 epochs, with 0.001 learning rate and categorical cross entropy.
"""

model = tf.keras.models.Sequential([                                                              
  ### Flatten
  tf.keras.layers.Flatten(input_shape=(256,256,3)),
  ### Dense
  tf.keras.layers.Dense(512,activation='relu',name='layer1'),
  ### Dense
  tf.keras.layers.Dense(256,activation='relu',name='layer2'),
  ### Dense
  tf.keras.layers.Dense(128,activation='relu',name='layer3'),
  ### Dense softmax
  tf.keras.layers.Dense(38,activation='softmax',name='layer_output')

])

model.compile(
    ### loss function sparse_categorical_crossentropy
    loss='sparse_categorical_crossentropy',
    ### Adam optimizer
    optimizer=tf.keras.optimizers.Adam(),
    ### metrics
    metrics=['sparse_categorical_accuracy','accuracy']
)

history = model.fit(
    pp_train,
    batch_size=128,
    epochs=10,
    validation_data=pp_val
)

"""#### Plot training & validation accuracy values"""

# Plot training & validation accuracy values
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

# Plot training & validation loss values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

"""#### Calculate test set"""

# Evaluate the model on the test data using `evaluate`
print('\n# Evaluate on test data')
results = model.evaluate(pp_test)
print('test loss, test acc:', results)

"""### (20 pts.) The Convolutional Neural Network implementation

#### It is a 4 layer Convolutional Neural Network. First two layers are Convolutional and last two layers are Fully Connected. The depths are [64,128,64] and the last softmax layer has 38 output. Train for 10 epochs, with 0.001 learning rate and categorical cross entropy.
"""

model_cnn = tf.keras.models.Sequential([
  ### Conv2D layer
  tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, activation='relu', input_shape=(256,256,3)),
  ### MaxPooling2D layer
  tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=1, padding='same'),
  ### Conv2D layer
   tf.keras.layers.Conv2D(filters=128, kernel_size=3, strides=1, activation='relu'),
  ### MaxPooling2D layer
  tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=1, padding='same'),
  ### Flatten
  tf.keras.layers.Flatten(),
  ### Dense
  tf.keras.layers.Dense(64, activation='relu'),
  ### Dense softmax
  tf.keras.layers.Dense(38, activation='softmax')
])

model_cnn.compile(
    ### loss function sparse_categorical_crossentropy
    loss='sparse_categorical_crossentropy',
    ### Adam optimizer
    optimizer=tf.keras.optimizers.Adam(),
    ### metrics
    metrics=['sparse_categorical_accuracy','accuracy']
)

history_cnn = model_cnn.fit(
    pp_train,
    epochs=10,
    validation_data=pp_val,
)

"""#### Plot training & validation accuracy values"""

# Plot training & validation accuracy values
plt.plot(history_cnn.history['accuracy'])
plt.plot(history_cnn.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

# Plot training & validation loss values
plt.plot(history_cnn.history['loss'])
plt.plot(history_cnn.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

"""#### Calculate test set"""

# Evaluate the model on the test data using `evaluate`
print('\n# Evaluate on test data')
results = model_cnn.evaluate(pp_test)
print('test loss, test acc:', results)

