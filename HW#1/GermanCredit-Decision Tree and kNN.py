# -*- coding: utf-8 -*-
"""ML-HW1-GermanCreditCard.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XvPgCtFbC0vIP7_4-EssriDkgKVkZCOR

# Load the dataset
"""

from google.colab import drive
drive.mount('/content/drive')

# You can find the data under https://drive.google.com/drive/folders/1e550az93U3_kfRBbVY5PZnMKYwGYmHqi?usp=sharing

import pandas as pd
import numpy as np

train_data = pd.read_csv("/content/drive/My Drive/HW1/train_data.csv")
train_label = pd.read_csv("/content/drive/My Drive/HW1/train_label.csv")

test_data = pd.read_csv("/content/drive/My Drive/HW1/test_data.csv")
test_label = pd.read_csv("/content/drive/My Drive/HW1/test_label.csv")

# show random samples from the training data
train_data.sample(5)
# One line of code

"""# Train Decision Tree with default parameters"""

from sklearn.tree import DecisionTreeClassifier

# Train decision tree using the whole training data with **entropy** criteria

decisionTree = DecisionTreeClassifier(criterion = "entropy")
decisionTree.fit(train_data,train_label)

# Estimate the prediction of test data
test_pred = decisionTree.predict(test_data)

# Calculate accuracy of test data
from sklearn.metrics import accuracy_score
TestAcc = accuracy_score(test_label,test_pred)
print("Testing Accuracy = %.5f%%" % (TestAcc * 100))

"""# FineTune Decision Tree parameters

1- Spliting dataset into train and validation
"""

# Split training data to 70% training and 30% validation
from sklearn.model_selection import train_test_split
x_train, x_val, y_train, y_val = train_test_split(train_data,train_label,test_size=0.3)

"""2- FineTune minimum sample split"""

min_samples_splits = range(2, 100)

train_results = []
val_results = []
for min_samples_split in min_samples_splits:
  
  # Fit the tree using the 70% portion of the training data
  decisionTree = DecisionTreeClassifier(min_samples_split=min_samples_split)
  decisionTree.fit(x_train,y_train)

  # Evaluate on Training set
  train_pred = decisionTree.predict(x_train)
  train_acc = accuracy_score(y_train,train_pred)
  train_results.append(train_acc)
   
  # Evaluate on Validation set
  val_pred = decisionTree.predict(x_val)
  val_acc = accuracy_score(y_val,val_pred)
  val_results.append(val_acc)
  
# Ploting
import matplotlib.pyplot as plt

plt.plot(min_samples_splits, train_results, 'b')
plt.plot(min_samples_splits, val_results,'r')
plt.show()

# Choose the best minimum split sample based on the plot
Best_minSampl = min_samples_splits[np.argmax(val_results)] 

# Train decision tree using the full training data and the best minimum split sample
decisionTree = DecisionTreeClassifier(min_samples_split=Best_minSampl)
decisionTree.fit(train_data,train_label)
# Estimate the prediction of the test data
test_pred = decisionTree.predict(test_data)

# Calculate accuracy of test data
TestAccDes = accuracy_score(test_label,test_pred)
print("Testing Accuracy = %.5f%%" % (TestAccDes * 100))

"""# Now, apply the same procedure but using KNN instead of decision tree 

# For finetuning, find the best value of K to use with this dataset.
"""

# Write your code here

from sklearn.neighbors import KNeighborsClassifier

# initialize the values of k to be a list of odd numbers between 1 and 30
kVals = range(1,30,2)

# Save the accuracies of each value of kVal in [accuracies] variable
# hint: you can use accuracies.append(...) function inside the loop
accuracies = []

# loop over values of k for the k-Nearest Neighbor classifier
for k in kVals:
  # Follow what we did in decision tree part
  kNN = KNeighborsClassifier(n_neighbors = k)
  kNN.fit(x_train,y_train)

  score = kNN.score(x_val, y_val)
  #print("For k = %d, validation accuracy = %.5f%%" % (k, score * 100))
  accuracies.append([score,k])

# Train KNN using the full training data with the best K that you found

kNN = KNeighborsClassifier(n_neighbors=int(max(accuracies,key=lambda x: x[0])[1]))
kNN.fit(train_data,train_label)

# Testing
test_pred = kNN.predict(test_data)
TestAcc = accuracy_score(test_label,test_pred)
print("Testing Accuracy = %.5f%%" % (TestAcc * 100))

"""# Bonus

# Apply gridsearch using decision tree on any hyperparameter(s) of your choice, you have to beat your previous obtained accuracies to get the bonus
"""

# Write your code here
from sklearn.model_selection import GridSearchCV

#min10
params = {'max_depth':list(range(1,20)),'min_samples_split':list(range(2,10)),'max_features':['auto','log2',None],'criterion':['entropy']}
grid_search_cv = GridSearchCV(DecisionTreeClassifier(), params, verbose=True,n_jobs = -1,cv=5)
grid_search_cv.fit(train_data,train_label)

grid_search_cv.best_estimator_

grid_search_cv.best_params_

grid_search_cv.best_score_

newTree=grid_search_cv.best_estimator_

newTree.fit(train_data,train_label)
test_pred = newTree.predict(test_data)
TestAccDesBonus = accuracy_score(test_label,test_pred)
print("Testing Accuracy = %.5f%%" % (TestAccDesBonus * 100))

import graphviz
from sklearn.tree import export_graphviz
# dot is a graph description language
dot = export_graphviz(newTree, out_file=None, 
                           feature_names=train_data.keys()) 

# we create a graph from dot source using graphviz.Source
graph = graphviz.Source(dot) 
graph

from sklearn.metrics import confusion_matrix

confusion_matrix(test_label,test_pred)

"""# Report: Write a summary of your approach to this problem; this should be like an abstract of a paper or the executive summary (you aim for clarity and passing on information, not going to details about known facts such as what decision trees are, assuming they are known to people in your research area).

Must include statements such as:


*   Include the problem definition: 1-2 lines
*   Talk about train/val/test sets, size and how split.
*   State what your test results are with the chosen method, parameters: e.g. "We have obtained the best results with the ….. classifier (parameters=....) , giving classification accuracy of …% on test data…."
*   Comment on the speed of the algorithms and anything else that you deem important/interesting (e.g. confusion matrix)

# Write your report in this cell

The main problem is obtaining a good accuracy for the decision tree model since the dataset is imbalanced. 

* It causes the machine learning model to be more biased towards majority class. 
* It causes poor classification of minority classes.

The minority class can be upsampled in order to solve the problem stated above.SMOTE(Synthetic Minority Oversampling Technique)
 
For validation step the algorithm GridSearchCV automatically uses StratifiedKFold().
* Train / Test size is 0.7 / 0.3 (a common splitting)
* Validation via StratifiedKFold() 
  * K = 5 

The dataset contains some irrelevant features such as '*id*' column where it does not contain any relationship with credit risk.

Hyperparameters Used for GridSearchCV and result regarding the best parameters:

'*{'criterion': 'entropy',
 'max_depth': 8,
 'max_features': None,
 'min_samples_split': 3}*'
 
 * criterion:
    * 'gini' and 'entropy' almost similiar but Gini is intended for continuous attributes, and Entropy for attributes that occur in classes.
    
 * max_depth:
  * If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.
  * Deeper the tree is allowed to grow,gets more complex and might overfit on train data which would work fine until the test data is used to inspect the performance metrics of the model.
 * max_features:
  * If not stated , max_feature = n_feature.
  * The search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than max_features features.
  * It affects efficiency majorly since every looking for a partition in every split is exhausting.
 * min_samples_split:
  * Used in order to set a value for minimum number of samples required to split an internal node.Effective in order to solve overfitting issues. 

Speed of the algorithm is correlated with the range of the parameters times number of K for Cross Validation times complexity of the building step and prediction step of model.Simply we can state that the range for all parameters as '*r*',number of iteration while cross validating as '*k*' and number of parameters to look as '*t*' we have $O(kr^t)$ for GridSearchCV without taking account the complexity of building of the model.Time complexity of the model would be $O(mn * log(n))$.
"""